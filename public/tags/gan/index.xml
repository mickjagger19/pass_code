<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>GAN on Mick&#39; Blog</title>
    <link>https://mickqian.github.io/tags/gan/</link>
    <description>Recent content in GAN on Mick&#39; Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Fri, 19 Jan 2024 20:27:05 +0800</lastBuildDate><atom:link href="https://mickqian.github.io/tags/gan/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>GAN &amp; Variants</title>
      <link>https://mickqian.github.io/posts/ai/models/gan--variants/</link>
      <pubDate>Fri, 19 Jan 2024 20:27:05 +0800</pubDate>
      
      <guid>https://mickqian.github.io/posts/ai/models/gan--variants/</guid>
      <description>Personal takeaways of GAN &amp;amp; its variants</description>
      <content:encoded><![CDATA[<h2 id="introduction">Introduction</h2>
<p>The mathematics in GAN and WGAN</p>
<h2 id="notations">Notations</h2>
<table>
  <thead>
      <tr>
          <th>Notation</th>
          <th>Meaning</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Jensen-Shannon Divergence</td>
          <td>An improved, symmetric version of  $KL$ Divergence:<br>$$D_{JS}(p||q) = \frac{1}{2} D_{KL}\left(p | \frac{p+q}{2}\right)+  \frac{1}{2} D_{KL}\left(q | \frac{p+q}{2}\right)$$</td>
      </tr>
  </tbody>
</table>
<h2 id="gan">GAN</h2>
<table>
  <thead>
      <tr>
          <th>Notation</th>
          <th>Meaning</th>
          <th></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>$p_{z}$</td>
          <td>distribution of noise input $z$</td>
          <td></td>
      </tr>
      <tr>
          <td>$p_{g}$</td>
          <td>distribution of generator output $x$</td>
          <td></td>
      </tr>
      <tr>
          <td>$p_{r}$</td>
          <td>distribution of real sample $x$</td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td></td>
          <td></td>
      </tr>
  </tbody>
</table>
<h3 id="arch">Arch</h3>
<p>The original GAN is an <strong>architecture</strong> (does not define the implementation details) consisting of two models:</p>
<ul>
<li>Discriminator $D$: Estimates the probability of <strong>a given sample comes from the real dataset</strong></li>
<li>Generator $G$: Generates synthetic samples with a noise variable input $z$ (for diversity)</li>
</ul>
<p>These two models compete against each other, forming an <strong>adversarial</strong> relationship.</p>
<h3 id="objective-design">Objective Design</h3>
<p>The objective function is defined as the combinations of performances of $D$ and $G$:</p>

$$
\mathcal{L}(D,G) = \underbrace{\mathbb{E}_{x \sim p_{r}(r)}[\log D(x)]}_{\text{for  D   to   identify  real  samples}} +  \underbrace{\mathbb{E}_{z \sim p_{z}(z)}[\log (1 - D(G(z)))]}_{\text{for  G  to  generate fake  samples}}
$$

<p>Using the same objective function, the direction of optimization varies:</p>
<ul>
<li>$D$: $D(x) \to 1, D(G(z)) \to 0$, so for $D$ it&rsquo;s <strong>maximize</strong></li>
<li>$G$: $p_{g} \to p_{r}$, since $D(p_{r}) \to 1$, so for $G$ it&rsquo;s <strong>minimize</strong></li>
</ul>
<blockquote>
<p>[!TIP] Other interpretations of objective function:</p>
<ul>
<li>The JS-Divergence of $p_{r}, p_{g}$: $G$ tries to minimize</li>
<li>Energy: $D$ tries to model energe-function $U(x)$, $G$ tries to generate sample $\hat{x}$ with (local) minimum energy $U(x)$</li>
</ul>
</blockquote>
<h3 id="optimal">Optimal</h3>
<p>We derive the optimal status for both models for xxx reasons.</p>
<h4 id="discriminator">Discriminator</h4>
<p>Rewrite objective function $\mathcal{L}(D,G)$:</p>

$$
\begin{align*}
\mathcal{L}(D,G) &= \mathbb{E}_{x \sim p_{r}}[\log D(x)] +  \mathbb{E}_{z \sim p_{z}}[\log (1 - D(G(z)))]\\\\
&= \mathbb{E}_{x \sim p_{r}}[\log D(x)] +  \mathbb{E}_{x \sim p_{g}}[\log (1 - D(x))]\\\\
 &= \int_{x}{p_{r}\log D(x) + p_{g}\log (1 - D(x))}& \text{actually L(D)}\\\\
\end{align*}
$$

<p>$$
\begin{align*}
&amp;\frac{d(L(D^{\ast}))}{ dD^{\ast}} = \frac{p_{r}}{D(x^{\ast})} + \frac{p_{g}}{1 - D(x^{\ast})} = 0\\
&amp;\Rightarrow D^{\ast}(x) = \frac{p_{r}}{p_{r} + p_{g}}\\
&amp;\Rightarrow D^{\ast}(x) = \frac{1}{2}
\end{align*}
$$</p>
<p>That is to say, when $G$ is trained to optimal, $D$ will output probability as 0.5</p>
<h4 id="generator">Generator</h4>
<p>$$
\mathcal{L}(G) = \mathcal{L}( D = D_{0}, G) = 2D_{JS}(p_{r}||p_{g}) - 2 \log 2
$$
So the target of training $G$ can be interpreted as increasing the distance between $p_{r}$ and generated distribution $p_{g}$:</p>

$$
\mathcal{L}( D, f^{\ast}) = \mathbb{E}_{x \sim p_{r}}[f(x) ] - \mathbb{E}_{z \sim p_{z}}[f(G(z))]
$$

<blockquote>
<p>[!TIP]
Almost in all GANs, for $G$, the objective can always be interpreted as (some kind of) difference between $p_{r}$ and $p_{g}$</p>
</blockquote>
<h3 id="problems-of-gan">Problems of GAN</h3>
<h4 id="hard-to-achieve-nash-equilibrium">Hard to achieve Nash Equilibrium</h4>
<p>Ideally, two models are trained simultaneously(or iteratively) to find a <a href="https://en.wikipedia.org/wiki/Nash_equilibrium"><strong>Nash Equilibrium</strong></a> to a two-player non-cooperative game. However, they update independently without coordination, so the convergence is not guaranteed.</p>
<p>















  
  
      
      
  <figure align=center class="figure d-block text-center">
  <picture align=center >
  <img class="figure-img img-fluid" src="https://mickqian.github.io/Attachments/AI/Models/GAN%20&%20Variants/IMG-20250104162106928.png?v=0a8592b3a6642df7c636096229480ad4#center" alt="A simulation of $D$ and $G$ to update regardless of each other" loading="lazy" height="550px" width="1305px" />
</picture>

  <figcaption class="figure-caption"><p>A simulation of $D$ and $G$ to update regardless of each other</p></figcaption>
</figure>
</p>
<blockquote>
<p>[!NOTE]
Maybe the idea of <strong>advesarial</strong> and <strong>cooperative</strong>(for a better overall result) are fundamentally contradictory</p>
</blockquote>
<h4 id="vanishing-gradient">Vanishing gradient</h4>
<p>When $D$ does a great job but $G$ does not, $D(x) \to 1, x \in p_{g}$, which is a constant value, this will lead to objective function being a fixed constants, thus the $\ell \to 0$.</p>
<p>















  
  
      
      
  <figure align=center class="figure d-block text-center">
  <picture align=center >
  <img class="figure-img img-fluid" src="https://mickqian.github.io/Attachments/AI/Models/GAN%20&%20Variants/IMG-20250104162107023.png?v=0a8592b3a6642df7c636096229480ad4" alt="The log_gradients of the generator decays quickly. Also, the better the $D$, the smaller the gradient" loading="lazy" height="666px" width="840px" />
</picture>

  <figcaption class="figure-caption"><p>The log_gradients of the generator decays quickly. Also, the better the $D$, the smaller the gradient</p></figcaption>
</figure>
</p>
<p>Hence, the ability of $D$ is faced with a dilemma: it can be neither too good or too gad.</p>
<h4 id="mode-collapse">Mode Collapse</h4>
<p>During training, the generator may collapse to a setting where it always produces same outputs. This is a common failure case for GANs, commonly referred to as <strong>Mode Collapse</strong>(or Mode Dropping).</p>
<blockquote>
<p>[!TIP]
The reason of mode collapse can be interpreted as: the generator finds a local optima where the loss-map is really sharp, leading to lack in diversity, due to the nature of KL-divergence</p>
</blockquote>
<h4 id="lack-of-a-reliable-evaluation-metric">Lack of a reliable evaluation metric</h4>
<p>The score given by $D$ does not provide an objective metric of the quality of image.</p>
<h2 id="sgan">SGAN</h2>
<table>
  <thead>
      <tr>
          <th style="text-align: left">Notations</th>
          <th style="text-align: left">Meaning</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">$\delta$ distribution: Dirac delta function</td>
          <td style="text-align: left">A function, whose value is zero everywhere except at zero, and $\int \delta  = 1$</td>
      </tr>
      <tr>
          <td style="text-align: left">$T$</td>
          <td style="text-align: left"></td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"></td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"></td>
      </tr>
  </tbody>
</table>
<p><strong>SGAN</strong> designs the Discriminator loss as:</p>
<p>$$
\begin{equation}\mathcal{D}<em>{SGAN}[p(x),q(x)]  =
\max</em>{T}, \frac{1}{2}\mathbb{E}<em>{x\sim p(x)}[\log \sigma(T(x))] + \frac{1}{2}\mathbb{E}</em>{x\sim q(x)}[\log (1 - \sigma(T(x)))] + \log 2</p>
<p>\end{equation}
$$
which is basically a <strong>dual form</strong> of JS-Divergence.</p>
<p>Consider a case, where $p(x)$ and $q(x)$ has no intervention:
$$
\begin{equation}p(x)=\delta(x-\alpha),q(x)=\delta(x-\beta)\end{equation}
$$
This changes loss function into:
$$
\begin{equation}\mathcal{D}[p(x),q(x)] = \max_T, \frac{1}{2}[\log \sigma(T(\alpha))] + \frac{1}{2}[\log (1 - \sigma(T(\beta)))] + \log 2\end{equation}
$$</p>
<p>And while training, $T(\alpha) \to +\infty, T(\beta) \to -\infty$</p>
<h2 id="wgan">WGAN</h2>
<table>
  <thead>
      <tr>
          <th>Notation</th>
          <th>Meaning</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><a href="https://en.wikipedia.org/wiki/Wasserstein_metric">Wasserstein Distance</a>(a.k.a. Earth Mover’s distance, short for EM distance) $\mathcal{W}$</td>
          <td>A measure of the distance between two PD, as the minimum energy cost of moving and transforming two PDs for them to be the same, a.k.a. the <strong>optimal transport cost</strong>. <br>$$\mathcal{W}(p,q) = \inf_{\gamma \sim \prod(p,q)}{\mathbb{E}_{(x,y) \sim \gamma}[c(x,y)]}$$</td>
      </tr>
      <tr>
          <td><a href="https://en.wikipedia.org/wiki/Lipschitz_continuity">Lipschitz continuity</a></td>
          <td>A strong form of uniform continuity for functions. If $\vert f(x_{1} - f(x_{2})\vert \le K\vert x_{1} - x_{2}\vert$, f is called $K$-Lipschitz, formally $\vert f\vert _{L} \le K$</td>
      </tr>
      <tr>
          <td>infimum $\inf$</td>
          <td>the greast lower  bound, in most cases the minimum</td>
      </tr>
      <tr>
          <td>supremum $\sup$</td>
          <td>the least upper bound, in most cases the maximum</td>
      </tr>
      <tr>
          <td>critic $f(.)$</td>
          <td>the form of $D$ in WGAN. No longer outputting the realism (a probability) of sample, it learns a specific distance (or a score, of distributions) from samples. $f(x) = D(x)$</td>
      </tr>
  </tbody>
</table>
<blockquote>
<p>[!TIP]
W-Distance is better than JS and KL for providing a smooth measure of distributions, even when they are disjoint</p>
</blockquote>
<p><strong>WGAN(Wasserstein Generative Adversarial Nets)</strong> is proposed to replace JS-divergence with W-distance, due its smooth-measurement nature.</p>
<p>Starting from the W-Distance of $p_{r}, p_{g}$, the objective function of WGAN is derived into the form:</p>

$$
\mathcal{L} = \mathcal{W}_{K}(p_{r}, p_{g}) = \sup_{f, |f|_{L} \le K}{\mathbb{E}_{x \sim p_{r}}[f(x) ] - \mathbb{E}_{x \sim p_{g}}[f(x)]}
$$

<p>WGAN consists of :</p>
<ul>
<li>$f(.)$ : Be used to approximate the W-Distance between two distributions to whom inputs belongs. It is trained by maximizing the objective function, formally, the subtraction of two scores. It is meant to find the maximum from all possible $f$</li>
<li>$G$: Generates samples. It is trained to minimize the objective function</li>
</ul>
<blockquote>
<p>[!TIP]</p>
<ul>
<li>In order to let critic $f$ subject to <strong>Lipschitz continuity</strong>, <strong>weight clipping</strong> is applied to <strong>indirectly</strong> achieve that.</li>
<li>Unlike original GAN, critic can be trained as good as possible but not leading to problems like <strong>gradient vanishing</strong> and <strong>mode collapse</strong></li>
</ul>
</blockquote>
<h2 id="wgan-gp">WGAN-GP</h2>
<p>Although improved, WGAN sometimes suffer from problems like <a href="/posts/ai/models/gan--variants/#hard-to-achieve-nash-equilibrium">Hard to achieve Nash Equilibrium</a>, still.</p>
<p>Identifying and proving <strong>weight clipping</strong> as a reason of that, authors of <strong>WGAN-GP</strong> replace it with <strong>gradient penalty(GP)</strong>:</p>

$$
\begin{align}
\mathcal{L} = \underbrace{\mathbb{E}_{\tilde x \sim p_{g}}[D(\tilde x)] - \mathbb{E}_{\tilde x \sim p_{r}}[D(x)]}_{\text{Original critic loss}} + \underbrace{\lambda\mathbb{E}_{\tilde x \sim p_{\tilde x }}[(||\nabla_{\hat x}(D(\hat x))||_{2}- 1)^2]}_{\text{gradient penalty}}
\end{align}
$$

<h2 id="gan-qp">GAN-QP</h2>
<p>Consider the following divergence: QP-div, quadratic potential divergence:
$$
\begin{equation}\begin{aligned}&amp;\mathcal{D}[p(x),q(x)] \ =&amp; \max_{T}, \mathbb{E}_{(x_r,x_f)\sim p(x_r)q(x_f)}\left[T(x_r,x_f)-T(x_f,x_r) - \frac{(T(x_r,x_f)-T(x_f,x_r))^2}{2\lambda d(x_r,x_f)}\right]\end{aligned}\end{equation}
$$
where:</p>
<ul>
<li>$\lambda &gt; 0$</li>
<li>$d$ is any distance function</li>
</ul>
<p>It&rsquo;s natural to build Loss Function for $D$ and $G$ with the Divergence, but for $G$, the quadratic term is neglected, since minimizing the denominator $d$  is not feasible.
$$
\begin{equation}\begin{aligned}&amp;T= \mathop{\arg\max}<em>T, \mathbb{E}</em>{(x_r,x_f)\sim p(x_r)q(x_f)}\left[T(x_r,x_f)-T(x_f,x_r) - \frac{(T(x_r,x_f)-T(x_f,x_r))^2}{2\lambda d(x_r,x_f)}\right] \ &amp;G = \mathop{\arg\min}<em>G,\mathbb{E}</em>{(x_r,x_f)\sim p(x_r)q(x_f)}\left[T(x_r,x_f)-T(x_f,x_r)\right]\end{aligned}\end{equation}
$$</p>
<h2 id="f-gan">f-GAN</h2>
<table>
  <thead>
      <tr>
          <th>Notation</th>
          <th>Meaning</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><a href="https://en.wikipedia.org/wiki/F-divergence">f-divergence</a></td>
          <td>A generalized version of <strong>KL-divergence</strong> and <strong>JS-divergence</strong>: $$\mathcal{D}_{f}(P|Q) = \int{q(x)f(\frac{p(x)}{q(x)})dx}$$, where $f$ is a convex function with measures the difference between PDs</td>
      </tr>
      <tr>
          <td>$T$</td>
          <td>The gradient of $f = \frac{p(x)}{q(x)}$</td>
      </tr>
      <tr>
          <td></td>
          <td></td>
      </tr>
  </tbody>
</table>
<p>In a nutshell, the $f$-GAN:</p>
<ul>
<li>Decide an $f$</li>
<li>Learn a model $\mathcal{D}$ to score the input $x \sim p_{r}, y \sim p_{g}$, whose results can be used to approximate the $f$-divergence between $p_{r}$ and $p_{g}$</li>
<li>Formulate the objective function as:
$$\mathcal{L} = \mathbb{E}_{x \sim P_{\text{data}}} [T^*(D(x))] - \mathbb{E}_{z \sim P_z} [f^*(T^*(D(G(z))))]$$</li>
</ul>
<h2 id="bigan">BiGAN</h2>
<p>Changes the task of $D$, $D(x, z)$ outputs the possibility of $x$ coming from real image.</p>
<p>Besides, the divergence of $p(x|z)$ and prior $\mathcal{N}(0, I)$ is no longer included in the loss, the target is achieved by optimizing $D$</p>
<p>$$
\mathcal{L} = (0 - D_{x\sim p_{g}, z\sim \mathcal{N}(0, I)}(x,z)))^{2} + (1 - D_{x\sim p_{r}, z\sim p(z|x)}(x,z)))^{2}
$$</p>
<h2 id="sagan">SAGAN</h2>
<p><strong>GAN</strong> fails to capture the invariant geometric pattern within some categories, e.g. the fur of dogs. This is not brought by GAN itself, since it doesn&rsquo;t imply any implementation, but brought by relying on <strong>Convolution</strong> to capture dependency between different regions.</p>
<p><strong>SAGAN</strong>(Self-Attention GAN) proproses two changes:</p>
<ul>
<li>Introduce <strong>self-attention</strong> into GAN: promotes the long-distance dependency and description of geometric features of image generation</li>
<li>Introduce <strong>spectral normalization</strong> into experiments, achieving better results</li>
<li>Set different learning rates for $G$ and $D$: $l_{r_{D}}$ = 4e-4, $l_{r_{G}}$ = 1e-4</li>
<li><strong>hinge adversarial loss</strong>: $L_{D} = -\mathbb{E}<em>{(x,y)\sim p</em>{data}}[min(0, -1 + D(x,y))]  -\mathbb{E}<em>{z\sim p</em>{z,}y \sim p_{data}}[min(0, -1 - D(G(z),y))]$, restricting ability of $D$</li>
</ul>
<h2 id="biggan">BigGAN</h2>
<h3 id="notations-1">Notations</h3>
<table>
  <thead>
      <tr>
          <th style="text-align: left">Notations</th>
          <th style="text-align: left">Meaning</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">Orthogonal matrix</td>
          <td style="text-align: left">A matrix, which row/column vectors are unit vector, and co-orthogonal. The reverse matrix of an orthogonal matrix is equal to its transpose matrix, $A^{T}A = AA^{T} = I$. The transpose of a real-valued orthogonal matrix is orthogonal.</td>
      </tr>
      <tr>
          <td style="text-align: left">Conjugate transpose</td>
          <td style="text-align: left">A matrix as a result of taking the conjugate of each elements, then transpose the original matrix. For real-valued matrixs, conjugate transpose is equal to transpose, since the conjugate of a real value is itself</td>
      </tr>
      <tr>
          <td style="text-align: left">Singular Value Decomposition(SVD)</td>
          <td style="text-align: left">An algorithm for decomposing matrix as <strong>eigenvector</strong> and <strong>eigenvalue</strong>, representing $A$ as: $A = U\Sigma V^{\ast}$, where $\Sigma$ is eigenvalue, $U$ is left eigenvector, $V$ is right eigenvector</td>
      </tr>
      <tr>
          <td style="text-align: left">Eigenvalue</td>
          <td style="text-align: left">Special value of a matrix, indicating the <strong>energy</strong> of a matrix in according <strong>eigenvector</strong> direction. In the direction of a eigenvector with bigger eigenvalue, data varies faster(e.g. the variation of the projected-matrix is bigger in the direction), so keeping the biggest eigenvalues is ideal in data compresion/dimension reduction</td>
      </tr>
      <tr>
          <td style="text-align: left">Singular Value</td>
          <td style="text-align: left">eigenvalue of the matrix</td>
      </tr>
      <tr>
          <td style="text-align: left">Spectral Norm (a.k.a. Induced 2-norm)</td>
          <td style="text-align: left">Maximum Singular value of the matrix:$$|A|<em>{2} = \max</em>{|x|<em>{2}\ne0} \frac{|Ax|</em>{2}}{|x|_2}$$By restricting spectral norm, Lipschitz can also be controlled.</td>
      </tr>
  </tbody>
</table>
<p><strong>BigGAN</strong> bases on <a href="/posts/ai/models/gan--variants/#sagan">SAGAN</a>, introduce several improvements:</p>
<ul>
<li>Send $z$ to multiple layers of $G$ with skip-net instead of input layer only</li>
<li><strong>Truncation trick</strong>: Truncate $z$ by setting threshold, exceeded sample is resampled until falling into the range</li>
<li>Hinge adversarial loss:</li>
<li>Spectral Norm</li>
</ul>
<h2 id="vae-gan">VAE-GAN</h2>
<p>The original output of VAE is blurry. By adding a discriminator, we hope it will recognize the blurs in the image.</p>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
